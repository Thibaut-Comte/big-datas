{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle linéaire\n",
    "\n",
    "Considérons la cas classique d'une fonction affine :\n",
    "\n",
    "$$y=ax+b$$\n",
    "\n",
    "Ici, $a$ et $b$ sont des réels. Ces deux nombres définissent entièrement la courbe et permet donc d'obtenir une relation **affine** entre $x$ et $y$. En statistique, cette relation est à la base des modèles dit **linéaires**, où une variable réponse se définit comme une somme de variables explicatives où chacune de ces dernières sont multipliés par un coefficient.\n",
    "\n",
    "\n",
    "## Modèle linéaire simple\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png)\n",
    "\n",
    "Dans le modèle linéaire simple (une seule variable explicative), on suppose que la variable réponse suit le modèle suivant :\n",
    "\n",
    "$$y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n",
    "\n",
    "On remarque la ressemblance avec la fonction affine présentée ci-dessus. La différence réside dans l'existence du terme aléatoire (appelé bruit) $\\varepsilon_i$. Afin de considérer le modèle, il est nécessaire de se placer sous les hypothèses suivantes\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\mathbb{E}[\\varepsilon_i]=0\\\\ \n",
    "\\text{Cov}(\\varepsilon_i, \\varepsilon_j)=\\delta_{ij} \\sigma^2\n",
    "\\end{matrix}\\right.$$\n",
    "Les différents éléments qui interviennent sont :\n",
    "\n",
    "- $\\beta_0$ : l'ordonnée à l'origine (nommée *intercept*)\n",
    "- $\\beta_1$ : le coefficient directeur\n",
    "- $x_i$ : l'observation $i$\n",
    "- $y_i$ : le $i$-ème prix\n",
    "- $\\varepsilon_i$ : le bruit aléatoire liée à la $i$-ème observation\n",
    "\n",
    "La solution peut se calculer facilement via les formules fermées suivantes :\n",
    "\n",
    "$$\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\qquad \\hat{\\beta}_0 = \\hat{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "## Modèle linéaire multiple\n",
    "\n",
    "Dans le cas multiple (pour $p$ variables explicatives), pour la $i$-ème observation, le modèle s'écrit :\n",
    "\n",
    "$$y_i= \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\varepsilon_i$$\n",
    "\n",
    "Ainsi, une observation $x_i$ n'est plus une valeur, mais un **vecteur** $(x_{i1}, \\dots, x_{ip})$. Il est plus commode de regrouper ces prix $y_i$ et ces vecteurs d'observations $x_i$ dans des matrices :\n",
    "\n",
    "$$Y=X \\beta + \\varepsilon$$\n",
    "\n",
    "Sous les hypothèses équivalentes du modèle simple en plus grand dimension\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\text{rank}(X)=p\\\\ \n",
    "\\mathbb{E}[\\varepsilon]=0 \\text{ et }\\text{Var}(\\varepsilon)=\\sigma^2 I_p\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "Les différents éléments qui interviennent sont :\n",
    "\n",
    "- $\\beta$ : le vecteur directeur\n",
    "- $X$ : la matrice des observations\n",
    "- $Y$ : le vecteur de prix\n",
    "- $\\varepsilon$ : le vecteur de bruit\n",
    "\n",
    "Avec $X=( \\mathbf{1}, X_1, \\dots, X_n)$, $Y=(y_1, \\dots, y_n)^\\top$ et $\\varepsilon=(\\varepsilon_1, \\dots, \\varepsilon_n)^\\top$. La solution des MCO (Moindres Carrés Ordinaires) est alors :\n",
    "\n",
    "$$\\hat{\\beta}= (X^\\top X)^{-1} X^\\top Y$$\n",
    "\n",
    "Vous pouvez d'ailleurs faire la démonstration de votre coté ! Pour plus d'information mathématiques, je vous conseil le portail de wikipédia qui est très bien fait : [lien ici](https://fr.wikipedia.org/wiki/Portail:Probabilit%C3%A9s_et_statistiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémenter une régression linéaire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer vos librairies \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn import linear_model \n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "#charger les données \n",
    "#price_availability.csv\n",
    "#listings_final.csv\n",
    "#attention aux valeurs manquantes !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données d'entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif ici est de charger les données pour créer les matrices $X$ et $Y$ du modèle linéaire. **Attention**, il n'est pas nécessaire de rajouter le vecteur colonne $\\mathbf{1}$ en première colonne, car *scikit-learn* le fait automatiquement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#définir vos variables de travail X et Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construire l'array des prix \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *Machine Learning*, on a l'habitude de couper l'ensemble de données en deux sous-ensembles :\n",
    "\n",
    "- Un ensemble d'entraînement (*train set*), sur lequel le modèle va être calibré.\n",
    "- Un ensemble de test (*test set*), qui ne sera pas utilisé pendant le calibrage mais permettra de vérifier l'aptitude du modèle à généraliser sur de nouvelles observations inconnues.\n",
    "\n",
    "En général, on découpe l'ensemble de données (*split*) en prenant $\\alpha \\%$ de l'ensemble pour entraînement et $1-\\alpha \\%$ comme test. Dans la plus part des cas, on considère que $\\alpha=10,20\\ ou \\ 30\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utiliser la méthode split de sklearn en splitant avec un alpha=30 et un random state=42 \n",
    "#afficher la shape de vos données \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour information, *scikit-learn* utilise le solveur OLS (Ordinary Least Squares) de *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#créer l'objet de régression et entrainer le sur notre ensemble d'entraînement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le vecteur des coefficients pour interpréter rapidement le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients beta_j : \n",
      " [2.47834609e-06 3.23451085e+01 1.43887639e+01 7.75318453e+01]\n",
      "Coefficients INTERCEPT beta_0 : \n",
      " -79.79953133672862\n"
     ]
    }
   ],
   "source": [
    "#afficher les coefficients\n",
    "#que remarquez vous ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation du modèle\n",
    "\n",
    "### Le coefficient de détermination $R^2$\n",
    "\n",
    "Par la suite, nous ferons l'hypothèse de gaussianité sur les bruits. Dans l'idée, nous aimerions obtenir une valeur numérique qui nous indique à quel point la régression linéaire a un sens sur nos données. Pour cela, introduisons les notations suivantes :\n",
    "\n",
    "- $SCT=\\|Y-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carrés totaux\n",
    "- $SCE=\\|\\hat{Y}-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carrés expliqués\n",
    "- $SCR=\\|\\hat{\\varepsilon}\\|^2$ est la somme des carrés résiduels\n",
    "\n",
    "L'idée est de décomposer la somme des carrés totaux comme la somme des carrés que le modèle explique, en plus de la somme des carrés qui sont liés aux résidus (et donc que le modèle ne peut pas expliquer). On voit donc ici l'intérêt de calculer un coefficient à partir du $SCE$. Puisque l'on a la relation suivante :\n",
    "\n",
    "$$SCT=SCE+SCR \\text{ alors } 1=\\frac{SCE}{SCT}+\\frac{SCR}{SCT}$$\n",
    "\n",
    "Plus les résidus sont petits (et donc la régression est \"bonne\"), plus $SCR$ devient petit et donc $SCE$ devient grand. Le schéma inverse s'opère de la même façon. Dans le meilleur des cas, on obtient $SCR=0$ et donc $SCE=SCT$ d'où le premier membre vaut $1$. Dans le cas contraite, $SCE=0$ et automatiquement, le premier membre est nul. C'est ainsi que l'on définit le coefficient de détermination $R^2$ comme \n",
    "$$R^2=\\frac{SCE}{SCT}=1-\\frac{SCR}{SCT}$$\n",
    "Ainsi, $R^2 \\in [0,1]$. Plus $R^2$ est proche de $1$, plus la régression linéaire a du sens. Au contraire, si $R^2$ est proche de $0$, le modèle linéaire possède un faible pouvoir explicatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du vecteur Y_pred : 999 \n"
     ]
    }
   ],
   "source": [
    "#faire une prediction sur X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 21320.77\n",
      "R2 : 0.37\n"
     ]
    }
   ],
   "source": [
    "#afficher l'erreur des moindres carrées sur l'ensemble d'entrainement ainsi que le R2\n",
    "#que remarquez vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
